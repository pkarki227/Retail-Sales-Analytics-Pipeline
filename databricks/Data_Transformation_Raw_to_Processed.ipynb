{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c717972e-6b87-47af-aad5-95db29a32db9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+-----------+-----+--------+----------+------+------------+\n|OrderID| OrderDate|     Product|   Category|Sales|Quantity|CustomerID|Region|TotalRevenue|\n+-------+----------+------------+-----------+-----+--------+----------+------+------------+\n|   1008|2024-01-16|Sony Headset|Electronics|  199|       2|   CUST008|   VIC|         398|\n|   1003|2024-01-12|Office Chair|  Furniture|  199|       2|   CUST003|   QLD|         398|\n|   1001|2024-01-10|   iPhone 14|Electronics| 1299|       1|   CUST001|   NSW|        1299|\n|   1005|2024-01-13|Dining Table|  Furniture|  599|       1|   CUST005|   VIC|         599|\n|   1004|2024-01-12| MacBook Pro|Electronics| 2499|       1|   CUST004|   NSW|        2499|\n|   1002|2024-01-11|  Samsung TV|Electronics|  899|       1|   CUST002|   VIC|         899|\n|   1006|2024-01-14| Dell Laptop|Electronics| 1099|       1|   CUST006|   QLD|        1099|\n|   1007|2024-01-15|   Bookshelf|  Furniture|  149|       1|   CUST007|    SA|         149|\n+-------+----------+------------+-----------+-----+--------+----------+------+------------+\n\nroot\n |-- OrderID: integer (nullable = true)\n |-- OrderDate: date (nullable = true)\n |-- Product: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- Sales: integer (nullable = true)\n |-- Quantity: integer (nullable = false)\n |-- CustomerID: string (nullable = true)\n |-- Region: string (nullable = true)\n |-- TotalRevenue: integer (nullable = true)\n\n✅ Cleaned CSV saved as: abfss://processed@retailanalyticslake.dfs.core.windows.net/retail_sales_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, col, expr\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Parse OrderDate correctly\n",
    "cleaned_df = retail_df.withColumn(\n",
    "    \"OrderDate\",\n",
    "    to_date(col(\"OrderDate\"), \"dd/MM/yyyy\")\n",
    ")\n",
    "\n",
    "# Cast Sales and Quantity to integer if needed\n",
    "cleaned_df = cleaned_df \\\n",
    "    .withColumn(\"Sales\", col(\"Sales\").cast(IntegerType())) \\\n",
    "    .withColumn(\"Quantity\", col(\"Quantity\").cast(IntegerType()))\n",
    "\n",
    "# Drop rows with null critical columns after date parsing\n",
    "cleaned_df = cleaned_df.na.drop(subset=[\"OrderID\", \"OrderDate\", \"Sales\", \"Quantity\", \"CustomerID\"])\n",
    "\n",
    "# Fill missing Quantity with 0 if any\n",
    "cleaned_df = cleaned_df.fillna({\"Quantity\": 0})\n",
    "\n",
    "# Remove duplicates\n",
    "cleaned_df = cleaned_df.dropDuplicates()\n",
    "\n",
    "# Add TotalRevenue column\n",
    "cleaned_df = cleaned_df.withColumn(\"TotalRevenue\", expr(\"Sales * Quantity\"))\n",
    "\n",
    "# Preview cleaned data\n",
    "cleaned_df.show()\n",
    "cleaned_df.printSchema()\n",
    "\n",
    "temp_path = \"abfss://processed@retailanalyticslake.dfs.core.windows.net/retail_sales_cleaned_csv_temp/\"\n",
    "cleaned_df.coalesce(1).write.format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .save(temp_path)\n",
    "\n",
    "# 4. Rename the part file to retail_sales_cleaned.csv\n",
    "files = dbutils.fs.ls(temp_path)\n",
    "csv_part_file = [f.path for f in files if f.name.startswith(\"part-\") and f.name.endswith(\".csv\")][0]\n",
    "final_csv_path = \"abfss://processed@retailanalyticslake.dfs.core.windows.net/retail_sales_cleaned.csv\"\n",
    "\n",
    "# Overwrite existing final file if exists\n",
    "dbutils.fs.rm(final_csv_path, True)\n",
    "dbutils.fs.cp(csv_part_file, final_csv_path)\n",
    "\n",
    "print(f\"✅ Cleaned CSV saved as: {final_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data_Transformation_Raw_to_Processed",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}